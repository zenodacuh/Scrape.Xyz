"""
MERCURY â€” Discord Bot
Deploy on Railway.
"""

import asyncio
import io
import json
import logging
import os
import random
import re
import time
from datetime import datetime, timezone
from pathlib import Path

import aiohttp
import discord
from discord.ext import commands, tasks
from discord import app_commands
from playwright.async_api import async_playwright

# â”€â”€â”€ CONFIG â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DISCORD_TOKEN      = os.environ["DISCORD_TOKEN"]
CHANNEL_ID         = int(os.environ["CHANNEL_ID"])
NEW_CHANNEL_ID     = int(os.environ["NEW_CHANNEL_ID"])
CONTENT_CHANNEL_ID = int(os.environ["CONTENT_CHANNEL_ID"])
TELEGRAM_TOKEN     = os.environ["TELEGRAM_TOKEN"]
TELEGRAM_CHAT      = os.environ["TELEGRAM_CHAT"]
OWNER_ID           = int(os.environ["OWNER_ID"])

CHECK_INTERVAL   = 10
PAGES_TO_SCAN    = 10
ARCHIVE_URL      = "https://pasteview.com/paste-archive"
SEEN_FILE        = "seen_urls.json"
EMPTY_SCAN_ALERT = 10
KEYWORDS         = ["hotmail", "hits", "mixed"]

# â”€â”€â”€ LOGGING â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%H:%M:%S",
)
log = logging.getLogger("mercury")

# â”€â”€â”€ STATE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
start_time = time.time()
stats      = {"total_pastes": 0, "total_combos": 0, "scans": 0, "empty_scans": 0}
scan_lock  = asyncio.Lock()

def load_seen() -> set:
    if Path(SEEN_FILE).exists():
        try:
            with open(SEEN_FILE) as f:
                return set(json.load(f))
        except Exception:
            pass
    return set()

def save_seen(seen: set):
    try:
        with open(SEEN_FILE, "w") as f:
            json.dump(list(seen), f)
    except Exception as e:
        log.error(f"Failed to save seen URLs: {e}")

posted_urls: set = load_seen()

# â”€â”€â”€ CREDENTIAL VALIDATION â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
EMOJI_RE = re.compile(
    "["
    u"\U0001F600-\U0001F64F"
    u"\U0001F300-\U0001F5FF"
    u"\U0001F680-\U0001F9FF"
    u"\U00002600-\U000027BF"
    u"\U0001FA00-\U0001FA6F"
    u"\U0001FA70-\U0001FAFF"
    u"\U00002702-\U000027B0"
    "]+", flags=re.UNICODE
)

JUNK_DOMAINS   = ("t.me", "telegram.me", "telegram.dog", "discord.gg", "http://", "https://", "lowza")
VALID_EMAIL_RE = re.compile(r"^[a-zA-Z0-9._%+\-]+@[a-zA-Z0-9.\-]+\.[a-zA-Z]{2,}$")

def is_valid_combo(line: str) -> bool:
    if not line or len(line) > 200:
        return False
    if "|" in line:
        return False
    if EMOJI_RE.search(line):
        return False
    if any(d in line.lower() for d in JUNK_DOMAINS):
        return False
    if ":" not in line:
        return False
    parts = line.split(":", 1)
    if len(parts) != 2:
        return False
    email, password = parts[0].strip(), parts[1].strip()
    if not password or len(password) < 3:
        return False
    if not VALID_EMAIL_RE.match(email):
        return False
    return True

def extract_credentials(raw: str) -> list[str]:
    seen = set()
    lines = []
    for line in raw.splitlines():
        line = line.strip()
        if line and is_valid_combo(line) and line not in seen:
            seen.add(line)
            lines.append(line)
    return lines

# â”€â”€â”€ TELEGRAM â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async def send_telegram_file(text: str, filename: str):
    url  = f"https://api.telegram.org/bot{TELEGRAM_TOKEN}/sendDocument"
    data = aiohttp.FormData()
    data.add_field("chat_id", TELEGRAM_CHAT)
    data.add_field("document", text.encode(), filename=filename, content_type="text/plain")
    try:
        async with aiohttp.ClientSession() as session:
            async with session.post(url, data=data) as resp:
                if resp.status != 200:
                    body = await resp.text()
                    log.error(f"Telegram API error {resp.status}: {body}")
                else:
                    log.info("Posted to Telegram")
    except Exception as e:
        log.error(f"Failed to send to Telegram: {e}")

# â”€â”€â”€ BOT â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
intents = discord.Intents.default()
bot  = commands.Bot(command_prefix="!", intents=intents)
tree = bot.tree

async def post_pastes(channel, pastes: list[dict]):
    if not pastes:
        return
    try:
        content  = "\n".join(item["url"] for item in pastes)
        filename = f"hotmail_{datetime.now(timezone.utc).strftime('%Y%m%d_%H%M%S')}.txt"
        await channel.send(file=discord.File(fp=io.BytesIO(content.encode()), filename=filename))
    except Exception as e:
        log.error(f"Failed to post file: {e}")

async def post_new_alerts(channel, pastes: list[dict]):
    for item in pastes:
        try:
            await channel.send(f"= DETECTED 1 NEW URL =\n{item['url']}")
            await asyncio.sleep(0.5)
        except Exception as e:
            log.error(f"Failed to post alert: {e}")

async def extract_raw(page, url: str) -> str:
    for attempt in range(2):
        try:
            await page.goto(url, wait_until="networkidle", timeout=15000)
            await page.wait_for_timeout(1500)

            raw = await page.evaluate("""
                () => {
                    if (window.ace) {
                        const editors = document.querySelectorAll('.ace_editor');
                        for (let ed of editors) {
                            try {
                                const val = ace.edit(ed).getValue();
                                if (val && val.trim()) return val;
                            } catch(e) {}
                        }
                    }
                    const edEl = document.querySelector('.ace_editor');
                    if (edEl && edEl.env && edEl.env.editor)
                        return edEl.env.editor.getValue();
                    return null;
                }
            """)

            if not raw or not raw.strip():
                await page.evaluate("""
                    () => {
                        const s = document.querySelector('.ace_scroller');
                        if (s) s.scrollTop = s.scrollHeight;
                    }
                """)
                await page.wait_for_timeout(800)
                lines = await page.query_selector_all("div.ace_line")
                raw   = "\n".join([(await l.text_content() or "").strip() for l in lines])

            if not raw or not raw.strip():
                pre = await page.query_selector("pre")
                if pre:
                    raw = await pre.text_content()

            if raw and raw.strip():
                return raw

        except Exception as e:
            log.error(f"Extract attempt {attempt+1} failed for {url}: {e}")
            if attempt == 0:
                await asyncio.sleep(2)

    return ""

# â”€â”€â”€ BACKGROUND TASK â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@tasks.loop(seconds=CHECK_INTERVAL)
async def monitor_loop():
    try:
        channel = bot.get_channel(CHANNEL_ID) or await bot.fetch_channel(CHANNEL_ID)
    except Exception as e:
        log.error(f"Could not get channel: {e}")
        return

    if scan_lock.locked():
        log.info("Scan already in progress, skipping this cycle")
        return

    async with scan_lock:
        stats["scans"] += 1
        log.info(f"Running scan #{stats['scans']}...")

        async with async_playwright() as p:
            browser = await p.chromium.launch(
                headless=True,
                args=["--no-sandbox", "--disable-setuid-sandbox", "--disable-dev-shm-usage"]
            )
            page = await browser.new_page()

            try:
                # â”€â”€ Step 1: load archive â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                for attempt in range(3):
                    try:
                        await page.goto(ARCHIVE_URL, wait_until="networkidle", timeout=30000)
                        await page.wait_for_timeout(2000)
                        break
                    except Exception as e:
                        log.warning(f"Archive load attempt {attempt+1} failed: {e}")
                        if attempt == 2:
                            log.error("Archive failed after 3 attempts, skipping scan")
                            return
                        await asyncio.sleep(3)

                # â”€â”€ Step 2: scrape pages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                found = []
                for page_num in range(1, PAGES_TO_SCAN + 1):
                    if page_num > 1:
                        navigated = False
                        buttons   = await page.query_selector_all("button")
                        for btn in buttons:
                            text = await btn.text_content()
                            if text and text.strip().lower() in ["next", ">", "Â»", "â†’", "â–¶"]:
                                disabled  = await btn.get_attribute("disabled")
                                aria_dis  = await btn.get_attribute("aria-disabled")
                                if disabled is not None or aria_dis == "true":
                                    break
                                await btn.click()
                                await page.wait_for_timeout(2000)
                                navigated = True
                                break
                        if not navigated:
                            break

                    matches = await page.evaluate("""
                        (keywords) => {
                            const results = [];
                            for (const a of document.querySelectorAll('a')) {
                                const text = (a.innerText || a.textContent || '').toLowerCase();
                                if (keywords.some(k => text.includes(k))) {
                                    const href = a.href;
                                    if (href
                                        && !href.includes('/paste-archive')
                                        && !href.includes('/new')
                                        && !href.endsWith('/')
                                        && href !== window.location.href) {
                                        results.push({
                                            title: (a.innerText || a.textContent || '').trim().replace(/\\s+/g, ' '),
                                            url: href
                                        });
                                    }
                                }
                            }
                            return results;
                        }
                    """, KEYWORDS)
                    log.info(f"Page {page_num}: {len(matches)} match(es)")
                    found.extend(matches)

                # Deduplicate within this run
                seen_this_run = set()
                pastes        = []
                for item in found:
                    if item["url"] not in seen_this_run:
                        seen_this_run.add(item["url"])
                        pastes.append(item)

                stats["total_pastes"] += len(pastes)

                # â”€â”€ Step 3: post all URLs to channel 1 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                await post_pastes(channel, pastes)

                # â”€â”€ Step 4: filter new pastes & mark seen â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                new_pastes = [p for p in pastes if p["url"] not in posted_urls]
                if not new_pastes:
                    stats["empty_scans"] += 1
                    log.info(f"No new pastes (empty streak: {stats['empty_scans']})")
                    if stats["empty_scans"] == EMPTY_SCAN_ALERT:
                        try:
                            owner = await bot.fetch_user(OWNER_ID)
                            await owner.send(f"âš ï¸ MERCURY: No new pastes in {EMPTY_SCAN_ALERT} consecutive scans.")
                        except Exception as e:
                            log.error(f"Failed to DM owner: {e}")
                    return

                stats["empty_scans"] = 0
                for p in new_pastes:
                    posted_urls.add(p["url"])
                save_seen(posted_urls)
                log.info(f"{len(new_pastes)} new paste(s) detected")

                # â”€â”€ Step 5: new URL alerts to channel 2 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                try:
                    new_channel = bot.get_channel(NEW_CHANNEL_ID) or await bot.fetch_channel(NEW_CHANNEL_ID)
                    await post_new_alerts(new_channel, new_pastes)
                except Exception as e:
                    log.error(f"Could not post to new channel: {e}")

                # â”€â”€ Step 6: extract creds & post to channel 3 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                try:
                    content_channel = bot.get_channel(CONTENT_CHANNEL_ID) or await bot.fetch_channel(CONTENT_CHANNEL_ID)
                    combined        = []

                    for item in new_pastes[:5]:
                        url = item["url"]
                        log.info(f"Extracting from {url}")
                        raw = await extract_raw(page, url)
                        if raw:
                            creds = extract_credentials(raw)
                            if creds:
                                combined.append("\n".join(creds))
                                stats["total_combos"] += len(creds)
                                log.info(f"âœ“ {len(creds)} valid combos from {url}")
                            else:
                                log.info(f"No valid combos in {url}")
                        else:
                            log.info(f"No content extracted from {url}")

                    if combined:
                        output      = "\n\n".join(combined)
                        ts          = datetime.now(timezone.utc).strftime('%Y%m%d_%H%M%S')
                        title_lower = " ".join(p["title"].lower() for p in new_pastes)

                        if "hotmail" in title_lower:
                            label = "hotmail"
                        elif "hits" in title_lower:
                            label = "hits"
                        elif "mix" in title_lower or "mixed" in title_lower:
                            label = "mix"
                        else:
                            label = "content"

                        filename = f"{label}_{ts}.txt"

                        # Discord
                        await content_channel.send(file=discord.File(fp=io.BytesIO(output.encode()), filename=filename))
                        log.info(f"Posted to Discord as {filename}")

                        # DM owner
                        try:
                            owner = await bot.fetch_user(OWNER_ID)
                            total = sum(len(b.splitlines()) for b in combined)
                            await owner.send(f"âœ… New {label.upper()} detected â€” {total} combos")
                        except Exception as e:
                            log.error(f"Failed to DM owner: {e}")

                        # Telegram
                        all_creds = [l for b in combined for l in b.splitlines() if l.strip()]
                        random.shuffle(all_creds)
                        tg_header = (
                            f"WAR CLOUD PRIVATE {label.upper()}\n"
                            "------------------------\n"
                            "https://t.me/+5Bqqamk3cpcxNDA0\n"
                            "https://t.me/+5Bqqamk3cpcxNDA0\n"
                            "https://t.me/+5Bqqamk3cpcxNDA0\n\n"
                        )
                        await send_telegram_file(tg_header + "\n".join(all_creds), filename)

                    else:
                        log.info("Nothing to post to content channel")

                except Exception as e:
                    log.error(f"Could not post to content channel: {e}")

            except Exception as e:
                log.error(f"Monitor loop error: {e}")
            finally:
                await browser.close()


@monitor_loop.before_loop
async def before_monitor():
    await bot.wait_until_ready()

# â”€â”€â”€ SLASH COMMANDS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@tree.command(name="scrape", description="Manually trigger a scrape right now")
@app_commands.describe(pages="Number of archive pages to scan (default: 5)")
async def cmd_scrape(interaction: discord.Interaction, pages: int = PAGES_TO_SCAN):
    await interaction.response.send_message(f"ğŸ”´ Scanning {pages} page(s)...", ephemeral=True)
    await monitor_loop()
    await interaction.followup.send("âœ… Done.", ephemeral=True)


@tree.command(name="stats", description="Show bot stats")
async def cmd_stats(interaction: discord.Interaction):
    uptime_secs          = int(time.time() - start_time)
    hours, remainder     = divmod(uptime_secs, 3600)
    minutes, seconds     = divmod(remainder, 60)
    embed = discord.Embed(title="MERCURY // STATS", color=0xCC0000, timestamp=datetime.now(timezone.utc))
    embed.add_field(name="Uptime",       value=f"{hours}h {minutes}m {seconds}s", inline=True)
    embed.add_field(name="Scans Run",    value=str(stats["scans"]),               inline=True)
    embed.add_field(name="Pastes Found", value=str(stats["total_pastes"]),        inline=True)
    embed.add_field(name="Combos Found", value=str(stats["total_combos"]),        inline=True)
    embed.add_field(name="URLs Tracked", value=str(len(posted_urls)),             inline=True)
    embed.add_field(name="Check Every",  value=f"{CHECK_INTERVAL}s",             inline=True)
    await interaction.response.send_message(embed=embed)

# â”€â”€â”€ EVENTS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@bot.event
async def on_ready():
    log.info(f"Logged in as {bot.user} (ID: {bot.user.id})")
    try:
        synced = await tree.sync()
        log.info(f"Synced {len(synced)} slash command(s)")
    except Exception as e:
        log.error(f"Failed to sync commands: {e}")
    if not monitor_loop.is_running():
        monitor_loop.start()
        log.info(f"Monitor started â€” checking every {CHECK_INTERVAL}s")
    else:
        log.info("Monitor already running after reconnect")

@bot.event
async def on_resumed():
    log.info("Discord session resumed")
    if not monitor_loop.is_running():
        monitor_loop.start()
        log.info("Monitor restarted after resume")

# â”€â”€â”€ RUN â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    bot.run(DISCORD_TOKEN, log_handler=None)
